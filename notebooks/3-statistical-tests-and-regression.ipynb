{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disparity Analysis and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "\n",
    "import istarmap  # import to apply monkey patch to multiprocess\n",
    "import multiprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy import stats \n",
    "from IPython.display import Markdown, display\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from aggregators import bucket_and_bin, filter_df\n",
    "from config import city2ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ouputs\n",
    "fn_distance = '../data/output/figs/fig2_dist.csv'\n",
    "fn_disparity = '../data/output/tables/table1_disparities_by_city.csv'\n",
    "fn_regression_all = '../data/output/tables/table2_regression_outputs_all.csv'\n",
    "fn_regression_income = '../data/output/tables/table3a_regression_outputs_income.csv'\n",
    "fn_regression_race = '../data/output/tables/table3b_regression_outputs_race.csv'\n",
    "fn_regression_redlining = '../data/output/tables/table3c_regression_outputs_redlining.csv'\n",
    "\n",
    "\n",
    "fn_observed_income = '../data/output/figs/fig4_income.csv'\n",
    "fn_adjusted_income = '../data/output/figs/fig5_income.csv'\n",
    "fn_observed_race = '../data/output/figs/fig4_race.csv'\n",
    "fn_adjusted_race= '../data/output/figs/fig5_race.csv'\n",
    "fn_observed_redlining = '../data/output/figs/fig4_redlining.csv'\n",
    "fn_adjusted_redlining = '../data/output/figs/fig5_redlining.csv'\n",
    "\n",
    "# inputs\n",
    "fn_att = '../data/output/speed_price_att.csv.gz'\n",
    "fn_centurylink = '../data/output/speed_price_centurylink.csv.gz'\n",
    "fn_verizon = '../data/output/speed_price_verizon.csv.gz'\n",
    "fn_earthlink = '../data/output/speed_price_earthlink.csv.gz'\n",
    "inputs = {\n",
    "    \"AT&T\" : fn_att,\n",
    "    \"CenturyLink\": fn_centurylink,\n",
    "    \"Verizon\": fn_verizon,\n",
    "    \"EarthLink\" : fn_earthlink\n",
    "}\n",
    "\n",
    "# params\n",
    "recalculate = False\n",
    "n_jobs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many cities and offers do we have?\n",
    "c = 0\n",
    "cities = set()\n",
    "for (isp, fn) in inputs.items():\n",
    "    d = filter_df(fn, isp=isp)\n",
    "    cities.update(d.major_city.unique())\n",
    "    c += len(d)\n",
    "    print(f\"{isp} we analyzed {len(d)} addresses from {d.major_city.nunique()} cities\")\n",
    "c, len(cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def which_has_largest_proportion(df, col='income_level', bins=['Low'], comparison=[], limit=300):\n",
    "    sample1 = df[df[col].isin(bins)]\n",
    "    if len(comparison) != 0:\n",
    "        sample2 = df[df[col].isin(comparison)]\n",
    "    else:\n",
    "        sample2 = df[~df[col].isin(bins)]\n",
    "        \n",
    "    if len(sample1) < limit or len(sample2) < limit:\n",
    "        return None, None, None, 'small bin'\n",
    "    \n",
    "    try:\n",
    "        prop1 = round(len(sample1[sample1.is_slow == 1]) / len(sample1), 4)\n",
    "        prop2 = round(len(sample2[sample2.is_slow == 1]) / len(sample2), 4)\n",
    "        return prop1, prop2, round(prop1 - prop2, 3) >= .05, None\n",
    "    except Exception as e:\n",
    "        print(e, df.head(2))\n",
    "        return None, None, None, e\n",
    "\n",
    "def has_disparity(df, isp, params, limitations=False):\n",
    "    \"\"\"\n",
    "    Checks for disparities in each city.\n",
    "    \"\"\"\n",
    "    iv = params['iv']\n",
    "    data = []\n",
    "    \n",
    "    s = df.groupby('major_city').race_perc_non_white.quantile([.25,.75]).unstack()\n",
    "    s['delta'] = s[0.75] - s[0.25]\n",
    "    race_cities = set(s[s.delta >= .2].index)\n",
    "    \n",
    "    for (city, state), _df in df.groupby(['major_city', 'state']):\n",
    "        uniform = False\n",
    "        _df = bucket_and_bin(_df, limitations=limitations)\n",
    "        \n",
    "        n_slow = len(_df[_df.is_slow == True])\n",
    "        n_not_slow = len(_df[_df.is_slow == False])\n",
    "        n_all = len(_df)\n",
    "        \n",
    "        variety = _df.speed_down_bins.value_counts(normalize=True)\n",
    "        variety[variety >= .95]\n",
    "        \n",
    "        if len(variety[variety >= .95]) != 0:\n",
    "            uniform = True\n",
    "            \n",
    "        if iv == 'income':\n",
    "            a, b, is_larger, flag = which_has_largest_proportion(_df, col=params['col'], bins=params['grades'],\n",
    "                                                                 comparison=params['comparison'])\n",
    "\n",
    "        elif iv == 'redlining':\n",
    "            _df_rated = _df[~_df.redlining_grade.isnull()]\n",
    "            if len(_df_rated) / n_all <= .05:\n",
    "                data.append({'major_city': city, 'state': state, 'isp': isp,\n",
    "                             f'prop_slow_{iv}_exposure': None,\n",
    "                             f'prop_slow_{iv}_treatment': None,\n",
    "                             f'prop_slow_{iv}_delta' : None,\n",
    "                             f'uniform_speed_{iv}': uniform,\n",
    "                             f'flag_{iv}': 'not HOLC graded'})\n",
    "                continue\n",
    "            a, b, is_larger, flag = which_has_largest_proportion(_df, col=params['col'], bins=params['grades'],\n",
    "                                                           comparison=params['comparison'])\n",
    "        elif iv == 'race':\n",
    "            n_minority_white = len(_df[_df.race_perc_non_white > .50])\n",
    "            n_majority_white = len(_df[_df.race_perc_non_white < .50])\n",
    "            # check only cities with at least 5 percent of addresses in minority white.\n",
    "            if (n_minority_white / n_all <= .05) or (n_majority_white / n_all <=.05):\n",
    "                data.append({'major_city': city, 'state': state, 'isp': isp,\n",
    "                         f'prop_slow_{iv}_exposure': None,\n",
    "                         f'prop_slow_{iv}_treatment': None,\n",
    "                         f'prop_slow_{iv}_delta' : None,\n",
    "                         f'uniform_speed_{iv}': uniform,\n",
    "                         f'flag_{iv}' : 'no diversity'})\n",
    "                continue\n",
    "            a, b, is_larger, flag = which_has_largest_proportion(_df, col=params['col'], bins=params['grades'],\n",
    "                                                          comparison=params['comparison'])\n",
    "\n",
    "        data.append({\n",
    "            'major_city': city,\n",
    "            'state': state,\n",
    "            f'slowest_{iv}' : is_larger,\n",
    "            'isp': isp,\n",
    "            f'prop_slow_{iv}_exposure': a,\n",
    "            f'prop_slow_{iv}_treatment': b,\n",
    "            f'prop_slow_{iv}_delta' : a-b if a else None,\n",
    "            f'uniform_speed_{iv}': uniform,\n",
    "            f'flag_{iv}': flag\n",
    "        })\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming columns, because I am not as smart as I think I am...\n",
    "col2colrename = {\n",
    "    'prop_slow_income_exposure': 'pct_slow_lower_income',\n",
    "    'prop_slow_income_treatment': 'pct_slow_upper_income',\n",
    "    'prop_slow_income_delta' : 'income_pct_pt_diff',\n",
    "    'prop_slow_race_exposure': 'pct_slow_least_white',\n",
    "    'prop_slow_race_treatment': 'pct_slow_most_white',\n",
    "    'prop_slow_race_delta' : 'race_pct_pt_diff',\n",
    "    'prop_slow_redlining_exposure': 'pct_slow_d_rated',\n",
    "    'prop_slow_redlining_treatment': 'pct_slow_ab_rated',\n",
    "    'prop_slow_redlining_delta' : 'redlining_pct_pt_diff',\n",
    "    'uniform_speed_income': 'uniform_speed',\n",
    "    'slowest_income': 'income_disparity',\n",
    "    'slowest_race': 'race_disparity',\n",
    "    'slowest_redlining': 'redlining_disparity'\n",
    "}\n",
    "\n",
    "back2colname = {v:k for k,v in col2colrename.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_params = {'iv': \"income\", 'grades': ['Low'], 'col': 'income_level'}\n",
    "race_params = {'iv': \"race\", 'grades': ['least white'], 'col': 'race_quantile'}\n",
    "redlining_params = {'iv': \"redlining\", 'grades': ['D'], 'col': 'redlining_grade'}\n",
    "\n",
    "# Treatment group\n",
    "income_params['comparison'] = ['Upper Income']\n",
    "race_params['comparison'] = ['most white']\n",
    "redlining_params['comparison'] = ['A', 'B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limitations = True # set to True for different categorization criteria for income and race/ethnicity\n",
    "\n",
    "if not os.path.exists(fn_disparity) or recalculate:\n",
    "    table = pd.DataFrame()\n",
    "    for isp, fn in tqdm(inputs.items()):\n",
    "        df = filter_df(fn, isp)\n",
    "        income_data = has_disparity(df, isp=isp, params=income_params, limitations=limitations)\n",
    "        race_data = has_disparity(df, isp=isp, params=race_params, limitations=limitations)\n",
    "        redlining_data = has_disparity(df, isp=isp, params=redlining_params, limitations=limitations)\n",
    "\n",
    "        df_table = pd.DataFrame(income_data).merge(\n",
    "            pd.DataFrame(race_data), on=['major_city', 'isp', 'state'], how='outer'\n",
    "        ).merge(\n",
    "            pd.DataFrame(redlining_data), on=['major_city', 'isp', 'state'], how='outer'\n",
    "        )\n",
    "\n",
    "        table = table.append(df_table)\n",
    "    table.major_city = table.major_city.str.title()\n",
    "    table.columns = [col2colrename.get(c, c) for c in table.columns]\n",
    "    table = table[[c for c in table.columns if c not in ['uniform_speed_race', 'uniform_speed_redlining']]]\n",
    "    if not limitations:\n",
    "        table.to_csv(fn_disparity, index=False)\n",
    "\n",
    "else:\n",
    "    table = pd.read_csv(fn_disparity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all cities in the study\n",
    "n_cities = table.major_city.nunique()\n",
    "n_cities\n",
    "trend = set(table[(table.income_disparity == True) | \n",
    "           (table.race_disparity == True) | \n",
    "           (table.redlining_disparity == True)].major_city.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many cities exhibit none of these trends?\n",
    "set(table[table.uniform_speed == False].major_city.unique()) - trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many cities have a redlining disparity that is 2x or greater?\n",
    "table['x_red'] = table.pct_slow_d_rated.div(table.pct_slow_ab_rated)\n",
    "redlined_2x_often = table[(table.x_red >= 2) & (table.uniform_speed == False)]\n",
    "\n",
    "redlined_2x_often.major_city.unique(), redlined_2x_often.major_city.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for params in [income_params, race_params ,redlining_params]:\n",
    "    iv = params['iv']\n",
    "    base_group = params['grades'][0]\n",
    "    \n",
    "    # filter out uniform cities here\n",
    "    table_ = table[table[f\"uniform_speed\"] == False]\n",
    "\n",
    "    # filter out cases we omitted for lack of data\n",
    "    null = table_[table_[f'{iv}_disparity'].isnull()]\n",
    "    null_city =  (set(table_[table_[f'{iv}_disparity'].isnull()].major_city.unique()) -\n",
    "                     set(table_[~table_[f'{iv}_disparity'].isnull()].major_city))\n",
    "    testable = table_[~table_[f'{iv}_disparity'].isnull()]\n",
    "    all_cities_tested = testable.major_city.nunique()\n",
    "    \n",
    "    # of significant cases, when is the slowest class the lower income/least white/redlined example\n",
    "    slowest = testable[testable[f\"{iv}_disparity\"] == True]\n",
    "    perc_slower= len(slowest)\n",
    "    perc_slower_city = slowest.major_city.nunique()\n",
    "    print(f\"{iv.title()} {len(testable)} city-ISP pairs and {all_cities_tested} cities\"\n",
    "          f\"\\n - {base_group} areas disproportionately offered slow speeds {perc_slower} ({perc_slower/len(testable)*100:.1f}%) city-isp pairs and {perc_slower_city} ({perc_slower_city/all_cities_tested*100:.1f}%) cities.\"\n",
    "          f\"\\n - omit {len(null)} city-isp-pairs and {len(null_city)} cities. \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(303)\n",
    "for isp in inputs.keys():\n",
    "    table_ = table[table.isp == isp]\n",
    "    if isp == 'EarthLink':\n",
    "        table_['major_city'] = table_.apply(lambda x: f\"{x['major_city']} ({x['state'].upper()})\", axis=1)\n",
    "        \n",
    "    null_cities = table_[table_.uniform_speed == True].major_city.unique()\n",
    "    table_ = table_[table_.uniform_speed == False]\n",
    "    all_cities = table_.major_city.nunique()\n",
    "    all_cities -= len(null_cities)\n",
    "\n",
    "    # income\n",
    "    n_cities = table_[~table_.income_disparity.isnull()].major_city.nunique()    \n",
    "    display(Markdown(f\"{isp} (N={n_cities}) disporportionate disparity of slow speeds in...\"))\n",
    "\n",
    "    yes_disparity = table_[table_.income_disparity== True].major_city.unique()\n",
    "    no_disparity = table_[table_.income_pct_pt_diff.between(-.05, .05, inclusive=\"neither\")].major_city.unique()\n",
    "    opposite_disparity = table_[table_.income_pct_pt_diff <= -.05].major_city.unique()\n",
    "    \n",
    "    random.shuffle(yes_disparity)\n",
    "    n_low = len(yes_disparity)\n",
    "\n",
    "    text =  (\n",
    "        f\"- {' and '.join(income_params['grades'])} Income areas than Upper Income areas in **{round(n_low / n_cities * 100, 1)}**% of cities ({n_low}).\"\n",
    "    )\n",
    "    if len(yes_disparity) > 1:\n",
    "        cities_ = ', '.join(yes_disparity[:3])\n",
    "        text += f\"<br>This includes cities such as: {cities_}.\"\n",
    "    if len(no_disparity) > 0:\n",
    "        perc = round(len(no_disparity) / n_cities * 100, 1)\n",
    "        text += f\"<br>No disparities in {perc}% of cities ({len(no_disparity)}).\"\n",
    "    if len(opposite_disparity) > 0:\n",
    "        perc = round(len(opposite_disparity) / n_cities * 100, 1)\n",
    "        text += f\"<br>The opposite trend was in {perc}% of cities ({len(opposite_disparity)}).\"\n",
    "    display(Markdown(text))\n",
    "\n",
    "    # race\n",
    "    n_cities = table_[~table_.race_disparity.isnull()].major_city.nunique()\n",
    "    cities = table_[table_.race_disparity == True].major_city.unique()\n",
    "    yes_disparity = table_[table_.race_disparity == True].major_city.unique()\n",
    "    no_disparity = table_[table_.race_pct_pt_diff.between(-.05, .05, \n",
    "                                                          inclusive=\"neither\")].major_city.unique()\n",
    "    opposite_disparity = table_[table_.race_pct_pt_diff <= -.05].major_city.unique()\n",
    "    random.shuffle(yes_disparity)\n",
    "    n_minority_white = len(yes_disparity)\n",
    "    \n",
    "    text = (\n",
    "        f\"- the {' and '.join(race_params['grades'])} areas compared to the most white areas in <b>{round(n_minority_white / n_cities * 100, 1)}</b>% of cities ({n_minority_white}).\"\n",
    "    )\n",
    "    if len(yes_disparity) > 1:\n",
    "        cities_ = ', '.join(yes_disparity[:3])\n",
    "        text += f\"<br>This includes cities such as: {cities_}.\"\n",
    "    if len(no_disparity) > 0:\n",
    "        perc = round(len(no_disparity) / n_cities * 100, 1)\n",
    "        text += f\"<br>No disparities in {perc}% of cities ({len(no_disparity)}).\"\n",
    "    if len(opposite_disparity) > 0:\n",
    "        perc = round(len(opposite_disparitiy) / n_cities * 100, 1)\n",
    "        text += f\"<br>The opposite trend was in {perc}% of cities ({len(opposite_disparitiy)}).\"\n",
    " \n",
    "    \n",
    "    if n_cities < all_cities:\n",
    "        diff = all_cities - n_cities\n",
    "        text += f\"<br>We exclude {diff} {'cities' if diff > 1 else 'city'} for not containing enough minority white areas for comparison.\"\n",
    "    display(Markdown(text))\n",
    "\n",
    "    # redlining\n",
    "    n_cities = table_[~table_.redlining_disparity.isnull()].major_city.nunique()\n",
    "    yes_disparity = table_[table_.redlining_disparity == True].major_city.unique()\n",
    "    no_disparity = table_[table_.redlining_pct_pt_diff.between(-.05, .05,\n",
    "                                                               inclusive=\"neither\")].major_city.unique()\n",
    "    opposite_disparity = table_[table_.redlining_pct_pt_diff <= -.05].major_city.unique()\n",
    "    random.shuffle(cities)\n",
    "    n_redline = len(cities)\n",
    "\n",
    "    text = (\n",
    "        f\"- {' and '.join(redlining_params['grades'])}\"\n",
    "        f\" graded areas than A and B graded areas in **{round(n_redline/ n_cities * 100, 1)}**% of cities ({n_redline}).\"\n",
    "    )\n",
    "    if len(yes_disparity) > 1:\n",
    "        cities_ = ', '.join(yes_disparity[:3])\n",
    "        text += f\"<br>This includes cities such as: {cities_}.\"\n",
    "    if len(no_disparity) > 0:\n",
    "        perc = round(len(no_disparity) / n_cities * 100, 1)\n",
    "        text += f\"<br>No disparities in {perc}% of cities ({len(no_disparity)}).\"\n",
    "    if len(opposite_disparity) > 0:\n",
    "        perc = round(len(opposite_disparity) / n_cities * 100, 1)\n",
    "        text += f\"<br>The opposite trend was statistically significant in {perc}% of cities ({len(opposite_disparity)}).\"\n",
    "    \n",
    "    if n_cities < all_cities:\n",
    "        diff = all_cities - n_cities\n",
    "        text += f\"<br>We exclude {diff} {'cities' if diff > 1 else 'city'} for not containing enough HOLC graded areas for comparison.\"\n",
    "    display(Markdown(text))\n",
    "\n",
    "    if len(null_cities) > 0:\n",
    "        n_null = len(null_cities)\n",
    "        if n_null == 1:\n",
    "            text = f\"Note: {null_cities[0].title()} was only offered slow speeds. We exclude this city from the above calculations.\"\n",
    "        else:\n",
    "            text =(\n",
    "                f\"Note: we exclude {n_null} {'cities' if n_null > 1 else 'city'} that are served the same speed.\"\n",
    "                f\" This includes {', '.join([c.title() for c in null_cities[:3]])}.\"\n",
    "            )\n",
    "        display(Markdown(text))\n",
    "    display(Markdown('<hr>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What patterns emerge when we collapose the ISPs together, and look at cities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.major_city.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_cities = table[(~table.income_disparity.isnull()) & (table.uniform_speed == False)].major_city.unique()\n",
    "race_cities = table[(~table.race_disparity.isnull()) & (table.uniform_speed == False)].major_city.unique()\n",
    "redlining_cities = table[(~table.redlining_disparity.isnull()) & (table.uniform_speed == False)].major_city.unique()\n",
    "\n",
    "all_cities = table[table.uniform_speed == False].major_city.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_1 = '#4f0d70'\n",
    "color_2 = \"#C31B6A\"\n",
    "\n",
    "bool2alpha = {\n",
    "    True: .95,\n",
    "    False : .35\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asterix_city(row, iv='income'):\n",
    "    if row[f'flag_{iv}'] == 'monospeed' or row[f'uniform_speed'] == True:\n",
    "        return city2ap.get(row['major_city'], row['major_city']) + '*'\n",
    "    if row[f'flag_{iv}'] in {'not HOLC graded', 'small bin',  'no diversity'}:\n",
    "        return city2ap.get(row['major_city'],row['major_city'])  + '˟'\n",
    "    return city2ap.get(row['major_city'], row['major_city'])\n",
    "\n",
    "def plot_observed(to_plot, iv='income', fn=None, title=None, ylim=60.5):    \n",
    "    to_plot['Observed'] = to_plot[f'{iv}_pct_pt_diff']\n",
    "    to_plot_ = to_plot[~to_plot['Observed'].isnull()].copy()\n",
    "#     to_plot_ = to_plot.copy()\n",
    "    to_plot_.sort_values(by=['isp', 'Observed'], ascending=[True, False], inplace=True)\n",
    "    to_plot_['display_city'] = to_plot_.apply(asterix_city, axis=1, iv=iv)\n",
    "    \n",
    "    cols = [\n",
    "        'Observed', \n",
    "        col2colrename.get(f'prop_slow_{iv}_exposure'), \n",
    "        col2colrename.get(f'prop_slow_{iv}_treatment')\n",
    "    ]\n",
    "    for col in cols:\n",
    "        to_plot_[col] = to_plot_[col] * 100\n",
    "    \n",
    "    to_plot_['major_city_isp'] = to_plot_.apply(lambda x: x['isp'] + ' - ' + x['display_city'], axis=1)\n",
    "\n",
    "    ax = to_plot_.plot(y='major_city_isp', x='Observed', \n",
    "                      color=color_1,\n",
    "                      kind='scatter', figsize=(6, 13))\n",
    "\n",
    "    to_plot_['isp'] = pd.Categorical(to_plot_['isp'], [\"AT&T\", \"Verizon\", \"CenturyLink\", \"EarthLink\"])\n",
    "    to_plot_ = to_plot_.sort_values(by=['isp', 'Observed'], \n",
    "                                    ascending=[True, False])\n",
    "    cols.extend(['isp', 'major_city', 'display_city'])\n",
    "    to_plot_[cols].to_csv(fn, index=False)\n",
    "    \n",
    "    to_plot_ = to_plot[(~to_plot['Observed'].isnull()) & (to_plot['uniform_speed'] == False)].copy()\n",
    "\n",
    "\n",
    "    # Hide the right and top spines\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "\n",
    "    ax.spines['top'].set_visible(False)\n",
    "\n",
    "    # Only show ticks on the left and bottom spines\n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    plt.gca().invert_yaxis()\n",
    "\n",
    "    ax.set_ylabel(\"\")\n",
    "    ax.set_xlabel(\"Percentage point difference\")\n",
    "\n",
    "    plt.axvline(x=5,linewidth=1, color='black', linestyle='--', ymax=.96)\n",
    "    plt.axvline(x=-5,linewidth=1, color='black', linestyle='--', ymax=.96)\n",
    "\n",
    "    plt.text(-8, -3., \n",
    "             title,\n",
    "             size=14,\n",
    "             horizontalalignment='left',\n",
    "             verticalalignment='center',);\n",
    "    ax.set_ylim(bottom=ylim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_observed(table, iv='income', fn=fn_observed_income,\n",
    "              ylim=57,\n",
    "              title = \"Compared to Upper Income areas,\\nLow income areas were disproportionately\\noffered slow speeds in almost every city\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_observed(table, iv='race', fn=fn_observed_race, ylim=50,\n",
    "              title = f\"Compared to Whiter counterparts,\\nthe Least White Areas were Disproportionately\\nOffered Slow Speeds in Almost Every City\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_observed(table, iv='redlining', fn=fn_observed_redlining, ylim=34,\n",
    "              title = f\"Compared to better HOLC-rated areas,\\nHistorically Redlined Areas were Disproportionately\\nOffered Slow Speeds in Every City\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_for_log_reg(df):\n",
    "    df['n_providers'] = df['n_providers'] - 1\n",
    "    df['income_dollars_below_median'] = df['income_dollars_below_median'] / 100\n",
    "\n",
    "    df['constant'] = 1\n",
    "    scaler = StandardScaler()\n",
    "    df['lat_standard'] = scaler.fit_transform(df[['lat']])[:, 0]\n",
    "    scaler = StandardScaler()\n",
    "    df['lon_standard'] = scaler.fit_transform(df[['lon']])[:, 0]\n",
    "    scaler = StandardScaler()\n",
    "    df['ppl_per_sq_mile_standard'] = scaler.fit_transform(df[['ppl_per_sq_mile']])[:, 0]\n",
    "    scaler = StandardScaler()\n",
    "    df['income_dollars_below_median'] = scaler.fit_transform(df[['income_dollars_below_median']])[:, 0]\n",
    "    \n",
    "    # if there are no competitors, make this variable null.\n",
    "    # Then this variable is linked to the outcome.\n",
    "    df.loc[df.n_providers < 1, 'internet_perc_broadband'] = None\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    df['internet_perc_broadband'] = scaler.fit_transform(df[['internet_perc_broadband']])[:, 0]\n",
    "\n",
    "    # this is our DV\n",
    "    df['is_slow'] = df.apply(\n",
    "        lambda x: 1 if x['speed_down_bins'] == \"Slow (<25 Mbps)\" else 0, \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    for iv in ['income_level', 'race_quantile', 'redlining_grade']:\n",
    "        df[iv] = df[iv].apply(lambda x: grade2rest.get(x, x))\n",
    "    return df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_return_coefs(formula, df, city, isp, model_name):\n",
    "    \"\"\"\n",
    "    Workflow for fitting binary logistic regression for `formula`.\n",
    "    This is formatted strangely because it'll be called by multiprocess(ing).Pool.\n",
    "    \"\"\"\n",
    "    import statsmodels.api as sm\n",
    "    import statsmodels.formula.api as smf\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    try:\n",
    "        results = smf.logit(formula=formula, data=df).fit(disp=0)\n",
    "        coefs = pd.DataFrame({\n",
    "            'coef': results.params.values,\n",
    "            'odds_ratio': np.exp(results.params.values),\n",
    "            'pvalue': results.pvalues,\n",
    "            'pr_sq': results.prsquared,\n",
    "            'N' : len(df),\n",
    "            'major_city': city,\n",
    "            'state' : df.state.iloc[0],\n",
    "            'isp': isp,\n",
    "            'model' : model_name,\n",
    "            'intercept': results.params['Intercept'],\n",
    "        })\n",
    "    except Exception as e:\n",
    "        coefs = pd.DataFrame([{\n",
    "            'N' : len(df),\n",
    "            'major_city': city,\n",
    "            'state' : df.state.iloc[0],\n",
    "            'isp': isp,\n",
    "            'model' : model_name,\n",
    "            'error': e\n",
    "        }])\n",
    "    \n",
    "    return coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def odds_to_probability(row, step=1):\n",
    "    \"\"\"\n",
    "    See about this equation here:\n",
    "    http://faculty.cas.usf.edu/mbrannick/regression/Logistic.html\n",
    "    \"\"\"\n",
    "    a = row['intercept']\n",
    "    b = row['coef']\n",
    "    X = step\n",
    "    try:\n",
    "        return math.exp(a + b * X) / (1 + math.exp(a + b * X))\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iv2treatment = {\n",
    "    \"income_level\" : 'Upper Income',\n",
    "    \"race_quantile\" : 'most white',\n",
    "    \"redlining_grade\" : 'rest'\n",
    "}\n",
    "\n",
    "grade2rest = {\n",
    "    'A' : 'rest',\n",
    "    'B' : 'rest',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(fn_regression_all) or recalculate:\n",
    "    data_regression = pd.DataFrame([])\n",
    "    \n",
    "    args = []\n",
    "    for isp, fn in inputs.items():\n",
    "        df = filter_df(fn, isp)\n",
    "        df['major_city'] = df['major_city'].apply(lambda x: x.title())\n",
    "        for (city, state), _df in df.groupby(by=['major_city', 'state']):\n",
    "            _df = bucket_and_bin(_df)\n",
    "            _df = preprocess_for_log_reg(_df)\n",
    "            \n",
    "            n_slow = len(_df[_df.speed_down_bins == 'Slow (<25 Mbps)'])\n",
    "            n_not_slow = len(_df[_df.speed_down_bins != 'Slow (<25 Mbps)'])\n",
    "            n_all = len(_df)\n",
    "            if (_df.speed_down_bins.nunique() <= 1) or (n_slow / n_all <= .01) or (n_not_slow / n_all <= .01):\n",
    "                # not enough speed\n",
    "                continue\n",
    "            for iv, treatment in iv2treatment.items():\n",
    "                if iv == 'race_quantile':\n",
    "                    n_minority_white = len(_df[_df.race_perc_non_white > .5])    \n",
    "                    n_majority_white = len(_df[_df.race_perc_non_white < .5])            \n",
    "\n",
    "                    # check only cities with at least 5 percent of addresses in minority white.\n",
    "                    if n_minority_white / len(_df) <= .05 or n_majority_white / len(_df) <= .05:\n",
    "                        print(f\"skip {city} {isp} race\")\n",
    "                        continue\n",
    "                elif iv == 'redlining_grade':\n",
    "                    _df_rated = _df[~_df.redlining_grade.isnull()]\n",
    "                    if len(_df_rated) <= len(_df) * .05:\n",
    "                        print(f\"skip {city} {isp} redline\")\n",
    "                        continue\n",
    "                    if 'D' not in _df['redlining_grade'].unique().tolist():\n",
    "                        print(f\"skip {city} {isp} redline\")\n",
    "                        continue\n",
    "\n",
    "                formula = f\"is_slow ~ C({iv}, Treatment('{treatment}'))\"\n",
    "                args.append([formula, _df, city, isp, f\"{iv}_alone\"])\n",
    "                if city in ['Omaha', 'Phoenix'] and iv == 'redlining_grade':\n",
    "                    formula += \" + ppl_per_sq_mile_standard + internet_perc_broadband\"\n",
    "                else:\n",
    "                    formula += \" + ppl_per_sq_mile_standard + n_providers + internet_perc_broadband\"\n",
    "                args.append([formula, _df, city, isp, f\"{iv}_controls\"])\n",
    "\n",
    "                # ablation formulas\n",
    "                if city in ['Omaha', 'Phoenix'] and iv == 'redlining_grade':\n",
    "                    formulas = [\" + ppl_per_sq_mile_standard\", \" + internet_perc_broadband\"]\n",
    "                else:\n",
    "                    formulas = [\" + ppl_per_sq_mile_standard\", \" + n_providers \",  \" + internet_perc_broadband\"]\n",
    "                for iv_ in formulas:\n",
    "                    formula = f\"is_slow ~ C({iv}, Treatment('{treatment}'))\"\n",
    "                    formula += \"\".join([f for f in formulas if f != iv_])\n",
    "                    model_name =  f\"{iv}_minus{iv_.replace(' + ', '_')}\"\n",
    "                    args.append([formula, _df, city, isp, model_name])\n",
    "\n",
    "    with multiprocess.get_context(\"spawn\").Pool(n_jobs) as pool:\n",
    "        for coefs in tqdm(pool.istarmap(train_and_return_coefs, args), \n",
    "                           total=len(args)):\n",
    "            data_regression = data_regression.append(coefs)\n",
    "\n",
    "    # treatment group IE upper income, most white, best graded\n",
    "    data_regression['probability_treatment'] = data_regression.apply(odds_to_probability, step=0, axis=1)\n",
    "    # exposure group IE lower income, least white, redlined\n",
    "    data_regression['probability_exposure'] = data_regression.apply(odds_to_probability, step=1, axis=1)\n",
    "    data_regression['odds_directional'] = data_regression.apply(lambda x: x['odds_ratio'] * -1 if x['coef'] <0 else  x['odds_ratio'], axis=1)\n",
    "    data_regression['is_coef_positive'] = data_regression.coef > 0\n",
    "    # positive if exposure group has higher liklihood of slower speeds than treatment group\n",
    "    data_regression['probability_delta'] = data_regression['probability_exposure'] - data_regression['probability_treatment']\n",
    "    data_regression['perc_probability'] =  data_regression['probability_exposure'] / data_regression['probability_treatment']\n",
    "    data_regression = data_regression[data_regression.index != 'Intercept']\n",
    "\n",
    "    data_regression.to_csv(fn_regression_all)\n",
    "    controls = data_regression[data_regression.model.str.contains('_controls')]\n",
    "    controls.loc[\"C(income_level, Treatment('Upper Income'))[T.Low]\"].to_csv(fn_regression_income)\n",
    "    controls.loc[\"C(race_quantile, Treatment('most white'))[T.least white]\"].to_csv(fn_regression_race)\n",
    "    controls.loc[\"C(redlining_grade, Treatment('rest'))[T.D]\"].to_csv(fn_regression_redlining)\n",
    "    \n",
    "else:\n",
    "    data_regression = pd.read_csv(fn_regression_all, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean psuedo r-squared (goodness of fit) for each model across cities and ISPs.\n",
    "for model, _df in data_regression.groupby(['model']):\n",
    "    mean_pr = _df.groupby(['major_city', 'state', 'isp']).first().pr_sq.mean()\n",
    "    print(model, mean_pr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ablated IV that results in the least change from the observed are the most influential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_ablation = data_regression[\n",
    "    (data_regression['model'].str.contains('income_level_minus_')) &\n",
    "    (data_regression.index == \"C(income_level, Treatment('Upper Income'))[T.Low]\") &\n",
    "    (data_regression.major_city.isin(income_cities))\n",
    "].merge(table, on=['isp', 'major_city', 'state'])\n",
    "income_ablation['delta'] = abs(income_ablation['probability_delta'] - income_ablation['income_pct_pt_diff'])\n",
    "income_ablation.sort_values(by='delta', ascending=True).groupby(['major_city', 'isp']).first().model.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_ablation = data_regression[\n",
    "    (data_regression['model'].str.contains('race_quantile_minus_')) &\n",
    "    (data_regression.index == \"C(race_quantile, Treatment('most white'))[T.least white]\") &\n",
    "    (data_regression.major_city.isin(race_cities))\n",
    "].merge(table, on=['isp', 'major_city', 'state'])\n",
    "race_ablation['delta'] = abs(race_ablation['probability_delta'] - race_ablation['race_pct_pt_diff'])\n",
    "race_ablation.sort_values(by='delta', ascending=True).groupby(['major_city', 'isp']).first().model.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redlining_ablation = data_regression[\n",
    "    (data_regression['model'].str.contains('redlining_grade_minus_')) &\n",
    "    (data_regression.index == \"C(redlining_grade, Treatment('rest'))[T.D]\") &\n",
    "    (data_regression.major_city.isin(redlining_cities))\n",
    "].merge(table, on=['isp', 'major_city', 'state'])\n",
    "redlining_ablation['delta'] = abs(redlining_ablation['probability_delta'] - redlining_ablation['redlining_pct_pt_diff'])\n",
    "redlining_ablation.sort_values(by='delta', \n",
    "                               ascending=False).groupby(['major_city', 'isp']).first().model.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where to disparities persist?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income = (\n",
    "    data_regression[\n",
    "        (data_regression.model == f'income_level_controls') &\n",
    "        (data_regression.index == \"C(income_level, Treatment('Upper Income'))[T.Low]\")\n",
    "    ].sort_values('coef', ascending=False)\n",
    "    .merge(\n",
    "        data_regression[\n",
    "            (data_regression.model == f'income_level_alone') &\n",
    "            (data_regression.index == \"C(income_level, Treatment('Upper Income'))[T.Low]\")\n",
    "        ], how='left', on=['major_city', 'state', 'isp'],\n",
    "        suffixes=['', '_alone']\n",
    "    )\n",
    ").drop_duplicates(subset=['isp', 'major_city', 'state'])\n",
    "income = income[income.major_city.isin(income_cities)]\n",
    "\n",
    "race = (\n",
    "    data_regression[\n",
    "        (data_regression.model == f'race_quantile_controls') &\n",
    "        (data_regression.index == \"C(race_quantile, Treatment('most white'))[T.least white]\")\n",
    "    ].sort_values(['coef', 'state'], ascending=False)\n",
    "    .merge(\n",
    "        data_regression[\n",
    "            (data_regression.model == f'race_quantile_alone') &\n",
    "            (data_regression.index == \"C(race_quantile, Treatment('most white'))[T.least white]\")\n",
    "        ], how='left', on=['major_city', 'state', 'isp'],\n",
    "        suffixes=['', '_alone']\n",
    "    )\n",
    ").drop_duplicates(subset=['isp', 'major_city', 'state'])\n",
    "race = race[race.major_city.isin(race_cities)]\n",
    "\n",
    "redlining = (\n",
    "    data_regression[\n",
    "        (data_regression.model == f'redlining_grade_controls') &\n",
    "        (data_regression.index == \"C(redlining_grade, Treatment('rest'))[T.D]\")\n",
    "    ].sort_values('coef', ascending=False)\n",
    "    .merge(\n",
    "        data_regression[\n",
    "            (data_regression.model == f'redlining_grade_alone') &\n",
    "            (data_regression.index == \"C(redlining_grade, Treatment('rest'))[T.D]\")\n",
    "        ], how='left', on=['major_city', 'state', 'isp'],\n",
    "        suffixes=['', '_alone']\n",
    "    )\n",
    ").drop_duplicates(subset=['isp', 'major_city', 'state'])\n",
    "redlining = redlining[redlining.major_city.isin(redlining_cities)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_isp_income = income[~income.pvalue.isnull()]\n",
    "slowest_income = city_isp_income[city_isp_income.probability_delta > 0.05]\n",
    "\n",
    "city_isp_race = race[~race.pvalue.isnull()]\n",
    "slowest_race = city_isp_race[city_isp_race.probability_delta > 0.05]\n",
    "\n",
    "city_isp_redlining = redlining[~redlining.pvalue.isnull()]\n",
    "slowest_redlining = city_isp_redlining[city_isp_redlining.probability_delta > 0.05]\n",
    "\n",
    "print(f\"Income ({len(city_isp_income)} city-ISP pairs and {city_isp_income.major_city.nunique()} cities) even after adjusting for other factors...\"\n",
    "     f\"\\n- lower income areas recieve greater proprotion of slow speeds in {len(slowest_income)} city-ISP pairs, or {len(slowest_income) / len(city_isp_income) * 100:.1f}% of cases.\\n\"\n",
    "     f\"- same pattern in {slowest_income.major_city.nunique()} out of {city_isp_income.major_city.nunique()} cities ({slowest_income.major_city.nunique() / city_isp_income.major_city.nunique():.2f}%)\")\n",
    "print(f\"Race and Ethnicity ({len(city_isp_race)} city-ISP pairs) even after adjusting for other factors...\"\n",
    "     f\"\\n- least white areas recieve greater proportion of slow speeds in {len(slowest_race)} city-ISP pairs, or {len(slowest_race) / len(city_isp_race) * 100:.1f}% of cases.\\n\"\n",
    "     f\"- same pattern in {slowest_race.major_city.nunique()} out of {city_isp_race.major_city.nunique()} cities ({slowest_race.major_city.nunique() / city_isp_race.major_city.nunique():.2f}%)\")\n",
    "\n",
    "print(f\"Redlining ({len(city_isp_redlining)} city-ISP pairs) even after adjusting for other factors...\"\n",
    "     f\"\\n- hazardous areas recieve greater proprotion of slow speeds in {len(slowest_redlining)} city-ISP pairs, or {len(slowest_redlining) / len(city_isp_redlining) * 100:.1f}% of cases.\\n\"\n",
    "     f\"- same pattern in {slowest_redlining.major_city.nunique()} out of {city_isp_redlining.major_city.nunique()} cities ({slowest_redlining.major_city.nunique() / city_isp_redlining.major_city.nunique():.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what percentage of AT&T cities do income disparities persist?\n",
    "income_att = income[income.isp == 'AT&T']\n",
    "len(income_att[income_att.probability_delta >= .05]) / len(income_att)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Misc model outputs for methodology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(income[income.pvalue < .05]) / len(income)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income.pr_sq.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income.odds_ratio.apply(lambda x: x>1.5).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(race[race.pvalue < .05]) / len(race)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race.pr_sq.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race.odds_ratio.apply(lambda x: x>1.5).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(redlining[redlining.pvalue < .05]) / len(redlining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redlining.pr_sq.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redlining.odds_ratio.apply(lambda x: x>1.5).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See the factor that lead to the greatest adjustments for any city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_ablation_ = data_regression[\n",
    "    (data_regression['major_city'] == 'Seattle') &\n",
    "    (data_regression['model'].str.contains('income_level_minus_')) & \n",
    "    (data_regression.index == \"C(income_level, Treatment('Upper Income'))[T.Low]\")\n",
    "].merge(table, on=['isp', 'major_city', 'state'])\n",
    "income_ablation_['delta'] = abs(income_ablation_['probability_delta'] - \n",
    "                                income_ablation_['income_pct_pt_diff'])\n",
    "income_ablation_.sort_values(by='delta', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_ablation_ = data_regression[\n",
    "    (data_regression['major_city'] == 'Charleston') &\n",
    "    (data_regression['model'].str.contains('race_quantile_minus_')) &\n",
    "    (data_regression.index == \"C(race_quantile, Treatment('most white'))[T.least white]\")\n",
    "].merge(table, on=['isp', 'major_city', 'state'])\n",
    "race_ablation_['delta'] = abs(race_ablation_['probability_delta'] - \n",
    "                              race_ablation_['race_pct_pt_diff'])\n",
    "race_ablation_.sort_values(by='delta', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redlining_ablation_ = data_regression[\n",
    "    (data_regression['major_city'] == 'Minneapolis') &\n",
    "    (data_regression['model'].str.contains('redlining_grade_minus_')) &\n",
    "    (data_regression.index == \"C(redlining_grade, Treatment('rest'))[T.D]\")\n",
    "].merge(table, on=['isp', 'major_city', 'state'])\n",
    "redlining_ablation_['delta'] = abs(redlining_ablation_['probability_delta'] - \n",
    "                                  redlining_ablation_['redlining_pct_pt_diff'])\n",
    "redlining_ablation_.sort_values(by='delta', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots with Adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2marker = {\n",
    "    'Observed': 'o',\n",
    "    'Adjusted': '^'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_adjusted(to_plot, ylim=33.5, figsize=(6, 8), fn=fn_adjusted_redlining,\n",
    "                  title= f\"After adjusting for other factors, Redlined areas\\nstill disprportionately offered slow Internet speeds\",\n",
    "                  title_y = -3.2, title_x= -.05):\n",
    "    legend_elements = [Line2D([0], [0], marker=label2marker.get(label), color='w', \n",
    "                          label=label, markerfacecolor=c, markersize=10)\n",
    "                   for label, c in {'Observed': color_1, 'Adjusted': color_2}.items()][::-1] \n",
    "    \n",
    "    to_plot_ = to_plot#[(to_plot['pvalue'] < 0.05)]\n",
    "    to_plot_['major_city_isp'] = to_plot_.apply(lambda x: x['isp'] + ' - ' + x['major_city'], axis=1)\n",
    "\n",
    "    to_plot_['Observed'] = to_plot_.probability_delta_alone * 100\n",
    "    to_plot_['Adjusted'] = to_plot_.probability_delta * 100\n",
    "\n",
    "    to_plot_ = to_plot_.sort_values(by=['Adjusted', 'state', 'Observed'], ascending=False)\n",
    "\n",
    "    ax = to_plot_.plot(y='major_city_isp', x='Observed', \n",
    "                      color=color_1,\n",
    "                      kind='scatter', figsize=figsize)\n",
    "    to_plot_.plot(y='major_city_isp', x='Adjusted', \n",
    "                 color=color_2, ax=ax, marker='^',\n",
    "                 alpha = [bool2alpha.get(_) for _ in to_plot_.pvalue < 0.05],\n",
    "                 kind='scatter')\n",
    "\n",
    "    x1 = to_plot_['Adjusted'].tolist()\n",
    "    x2 = to_plot_['Observed'].tolist()\n",
    "    n = to_plot_['major_city_isp'].tolist()\n",
    "    for i in range(0, len(x1)):\n",
    "        ax.plot([x1[i], x2[i]], [n[i], n[i]], '-', \n",
    "                color='black', zorder=-1, alpha=.8)\n",
    "\n",
    "    to_plot_['isp'] = pd.Categorical(to_plot_['isp'], [\"AT&T\", \"Verizon\", \"CenturyLink\", \"EarthLink\"])\n",
    "    to_plot_ = to_plot_.sort_values(by=['isp','Adjusted', 'state', 'Observed'], \n",
    "                                    ascending=[True, False, False, False])\n",
    "    to_plot_[['major_city', 'isp', 'Adjusted', 'Observed', 'state']].to_csv(fn, index=False)    \n",
    "\n",
    "    # Hide the right and top spines\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "\n",
    "    ax.spines['top'].set_visible(False)\n",
    "\n",
    "    # Only show ticks on the left and bottom spines\n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    plt.gca().invert_yaxis()\n",
    "\n",
    "    ax.set_ylabel(\"\")\n",
    "    ax.set_xlabel(\"Percentage point difference\")\n",
    "\n",
    "    plt.axvline(x=0,linewidth=1, color='black', linestyle='--', ymax=.96)\n",
    "\n",
    "    plt.text(title_x, title_y, \n",
    "             title,\n",
    "             size=14,\n",
    "             horizontalalignment='left',\n",
    "             verticalalignment='center',);\n",
    "    ax.set_ylim(bottom=ylim)\n",
    "\n",
    "    ax.legend(handles=legend_elements[::-1],\n",
    "                  loc='lower left', \n",
    "                  bbox_to_anchor= (-0.025, .97), \n",
    "                  ncol=5,\n",
    "                  handletextpad=0.0,\n",
    "                  labelspacing=0, \n",
    "                  borderaxespad=.1, \n",
    "                  borderpad=0.1,\n",
    "                  frameon=False,\n",
    "                  prop={'size': 9.2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot = income\n",
    "plot_adjusted(to_plot, ylim=58.5, figsize=(6, 10), \n",
    "              fn=fn_adjusted_income,\n",
    "              title_y = -4.2, title_x = -14, \n",
    "              title= f\"After adjusting for other factors, low income areas\\nstill disporportionately offered slow Internet speeds\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot = race\n",
    "plot_adjusted(to_plot, ylim=52.5, figsize=(6, 10), \n",
    "              fn=fn_adjusted_race,\n",
    "              title_y = -3.5, title_x = -15,\n",
    "              title= f\"After adjusting for other factors, least white areas\\nstill disproportionately offered slow Internet speeds\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot = redlining\n",
    "plot_adjusted(to_plot, title_y=-3.2, title_x=-6, ylim=37.5, fn=fn_adjusted_redlining)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rates per plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_rates = '../data/output/figs/story_rates.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isp_rates = []\n",
    "for isp, fn in inputs.items():\n",
    "    df_ = filter_df(fn, isp)\n",
    "    # remove no service and affordable plans\n",
    "    df_ = df_[(df_.speed_down != 0) & (df_.price > 30)]\n",
    "    df_['std_rate'] = df_['price'] / df_['speed_down']\n",
    "    rate_lowest = df_['std_rate'].min()\n",
    "    rate_highest = df_['std_rate'].max()\n",
    "    \n",
    "    isp_rates.append({\n",
    "        'isp' : isp,\n",
    "        'rate_lowest': rate_lowest,\n",
    "        'rate_highest': rate_highest,\n",
    "        'price' : df_['price'].iloc[0]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isp_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(isp_rates).to_csv(fn_rates, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
