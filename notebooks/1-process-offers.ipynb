{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing and Processing Lookup Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import gzip\n",
    "import json\n",
    "from datetime import datetime\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "from config import (\n",
    "    inc_city_att, \n",
    "    inc_city_cl, \n",
    "    inc_city_verizon, \n",
    "    inc_city_el\n",
    ")\n",
    "from parsers import (\n",
    "    cl_workflow, \n",
    "    att_workflow, \n",
    "    verizon_workflow, \n",
    "    el_workflow, \n",
    "    get_incorporated_places, \n",
    "    check_redlining, \n",
    "    get_holc_grade, \n",
    "    get_closest_fiber\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "fn_acs = '../data/intermediary/census/aggregated_tables_plus_features.csv.gz'\n",
    "pattern_att = '../data/intermediary/isp/att/*/*.geojson.gz' # pattern for all data collected from lookup tools\n",
    "pattern_cl = '../data/intermediary/isp/centurylink/*/*.geojson.gz'\n",
    "pattern_verizon = '../data/intermediary/isp/verizon/*/*.geojson.gz'\n",
    "pattern_el = \"../data/intermediary/isp/earthlink/*/*.geojson.gz\"\n",
    "\n",
    "# outputs\n",
    "fn_att = \"../data/output/speed_price_att.csv.gz\"\n",
    "fn_cl = '../data/output/speed_price_centurylink.csv.gz'\n",
    "fn_verizon = '../data/output/speed_price_verizon.csv.gz'\n",
    "fn_el = '../data/output/speed_price_earthlink.csv.gz'\n",
    "\n",
    "# params\n",
    "n_jobs = 20\n",
    "recalculate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is from Census data we crunched in the previous notebook.\n",
    "acs = pd.read_csv(fn_acs, dtype={'geoid': str, 'block_group': str})\n",
    "\n",
    "# These are the columns we're going to bring to merge with lookup responses.\n",
    "acs_cols = [\n",
    "    'geoid', 'race_perc_non_white','income_lmi', \n",
    "    'ppl_per_sq_mile', 'n_providers', 'income_dollars_below_median',\n",
    "    'internet_perc_broadband', 'median_household_income'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total data collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_addresses(fn):\n",
    "    \"\"\"\n",
    "    How many addresses did we successfully collect in each file?\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    with gzip.open(fn, 'rb') as f:\n",
    "        for line in f.readlines():\n",
    "            record = json.loads(line)\n",
    "            if record['collection_status'] != 0:\n",
    "                count += 1\n",
    "    return count \n",
    "\n",
    "def count_successful_addresses(pattern, n_jobs=20):\n",
    "    \"\"\"\n",
    "    For all files in `pattern`, sees how many addresses were successfully counted.\n",
    "    Uses multiprocessing to speed things up.\n",
    "    \"\"\"\n",
    "    files = glob.glob(pattern)\n",
    "    count = 0\n",
    "    with Pool(n_jobs) as pool:\n",
    "        for _count in tqdm(pool.imap_unordered(count_addresses, files), \n",
    "                           total=len(files)):\n",
    "            count += _count\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12652/12652 [00:16<00:00, 765.89it/s] \n",
      "100%|██████████| 10026/10026 [00:04<00:00, 2282.08it/s]\n",
      "100%|██████████| 5891/5891 [00:05<00:00, 1111.69it/s]\n",
      "100%|██████████| 17263/17263 [00:20<00:00, 825.78it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AT&T: 458787\n",
      "Verizon: 312357\n",
      "CenturyLink: 245139\n",
      "EarthLink: 590412\n",
      "Total: 1606695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "att_count = count_successful_addresses(pattern_att, n_jobs=n_jobs)\n",
    "verizon_count = count_successful_addresses(pattern_verizon, n_jobs=n_jobs)\n",
    "cl_count = count_successful_addresses(pattern_cl, n_jobs=n_jobs)\n",
    "el_count = count_successful_addresses(pattern_el, n_jobs=n_jobs)\n",
    "all_records = att_count + verizon_count + cl_count + el_count \n",
    "\n",
    "print(f\"\"\"AT&T: {att_count}\n",
    "Verizon: {verizon_count}\n",
    "CenturyLink: {cl_count}\n",
    "EarthLink: {el_count}\n",
    "Total: {all_records}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions we're going to be using\n",
    "We calcualte the distance to the closest household with Fiber and check HOLC grades using functions defined in `parsers.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mget_closest_fiber\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "\u001b[0;32mdef\u001b[0m \u001b[0mget_closest_fiber\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"\u001b[0m\n",
       "\u001b[0;34m    Convert coordinates to radians and fit a sklearn ball tree \u001b[0m\n",
       "\u001b[0;34m    to find closest household with 200 Mbps speeds.\u001b[0m\n",
       "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0m_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspeed_down\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;31m# create a ball tree just on fiber households\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBallTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeg2rad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lon'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"haversine\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;31m# find the closest fiber for every household\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdistances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeg2rad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lon'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                                    \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_distance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"closest_fiber_miles\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistances\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m3958.756\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;31m# merge the info of the closest fiber household\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mclosest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m'address_full'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lon'\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                    \u001b[0mleft_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                    \u001b[0msuffixes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_closest_fiber'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      ~/code/1_production-grade/isp/notebooks/parsers.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "??get_closest_fiber"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We `check_redlining` grades by looking if an addresses' coordinates (converted to a Shapely `Point`) are within the `Polygon`s of redlining maps by Mapping Inequality. This actual check is done by `get_holc_grade`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mget_holc_grade\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolygons\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "\u001b[0;32mdef\u001b[0m \u001b[0mget_holc_grade\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                   \u001b[0mpolygons\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"\u001b[0m\n",
       "\u001b[0;34m    Converts any lat and lon in a dictionary into a shapely point,\u001b[0m\n",
       "\u001b[0;34m    then iterate through a list of dictionaries containing \u001b[0m\n",
       "\u001b[0;34m    shapely polygons shapes for each HOLC-graded area.\u001b[0m\n",
       "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lon'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lat'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mfor\u001b[0m \u001b[0mpolygon\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpolygons\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mpolygon\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'shape'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0mpolygon\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'grade'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      ~/code/1_production-grade/isp/notebooks/parsers.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "??get_holc_grade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ATT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(fn_att) or recalculate:\n",
    "    # find the data we collected for each block group.\n",
    "    data_att = []\n",
    "    files = glob.glob(pattern_att)\n",
    "    with Pool(n_jobs) as pool:\n",
    "        # create parallel jobs that parse each block group of data using `att_workflow`.\n",
    "        for record in tqdm(pool.imap_unordered(att_workflow, files), \n",
    "                           total=len(files)):\n",
    "            data_att.extend(record)\n",
    "    att = pd.DataFrame(data_att)\n",
    "    del data_att\n",
    "    \n",
    "    # only keep addresses in the incorporated city\n",
    "    att = att[att.incorporated_place.isin(inc_city_att)]\n",
    "    att['block_group'] = att['block_group'].apply(lambda x: f\"{int(x):012d}\")\n",
    "    \n",
    "    # check HOLC-grades for each address, and the distance to download speeds at or above 200 Mbps\n",
    "    att = check_redlining(att)\n",
    "    att = get_closest_fiber(att)\n",
    "    \n",
    "    # merge census data, and save the file\n",
    "    att_acs = att.merge(acs[acs_cols], how='left',\n",
    "                        left_on='block_group', right_on='geoid')\n",
    "    att_acs.to_csv(fn_att, index=False, compression='gzip')\n",
    "else:\n",
    "    att_acs = pd.read_csv(fn_att)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[datetime.datetime(2022, 4, 18, 2, 18, 2),\n",
       " datetime.datetime(2022, 4, 27, 20, 47, 51)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start and end collection datetime\n",
    "[datetime.fromtimestamp(att_acs.collection_datetime.min()), \n",
    " datetime.fromtimestamp(att_acs.collection_datetime.max())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "states.extend(att_acs['state'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centurylink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(fn_cl) or recalculate:\n",
    "    data_cl = []\n",
    "    files = glob.glob(pattern_cl)\n",
    "    with Pool(n_jobs) as pool:\n",
    "        for record in tqdm(pool.imap_unordered(cl_workflow, files), \n",
    "                           total=len(files)):\n",
    "            data_cl.extend(record)\n",
    "    cl = pd.DataFrame(data_cl)\n",
    "    del data_cl\n",
    "    \n",
    "    cl = cl[cl['incorporated_place'].isin(inc_city_cl)]\n",
    "    cl = cl[cl.speed_down != 940]\n",
    "    \n",
    "    cl = check_redlining(cl)\n",
    "    cl = get_closest_fiber(cl)\n",
    "    \n",
    "    cl_acs = cl.merge(acs[acs_cols], how='left', \n",
    "                      left_on='block_group', right_on='geoid')\n",
    "    cl_acs.to_csv(fn_cl, index=False, compression='gzip')\n",
    "else:\n",
    "    cl_acs = pd.read_csv(fn_cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[datetime.datetime(2022, 4, 15, 18, 4, 41),\n",
       " datetime.datetime(2022, 4, 17, 17, 34, 25)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start and end collection datetime\n",
    "[datetime.fromtimestamp(cl_acs.collection_datetime.min()), \n",
    " datetime.fromtimestamp(cl_acs.collection_datetime.max())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "states.extend(cl_acs['state'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chino/miniconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3331: DtypeWarning: Columns (18) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(fn_verizon) or recalculate:\n",
    "    data_verizon = []\n",
    "    files = glob.glob(pattern_verizon)\n",
    "    with Pool(n_jobs) as pool:\n",
    "        for record in tqdm(pool.imap_unordered(verizon_workflow, files), \n",
    "                           total=len(files)):\n",
    "            data_verizon.extend(record)\n",
    "    verizon = pd.DataFrame(data_verizon)\n",
    "    del data_verizon\n",
    "    \n",
    "    verizon = verizon[verizon.incorporated_place.isin(inc_city_verizon)]\n",
    "    \n",
    "    verizon['lon'] = verizon['lon'].astype(float)\n",
    "    verizon['lat'] = verizon['lat'].astype(float)    \n",
    "    verizon = check_redlining(verizon)\n",
    "    verizon = get_closest_fiber(verizon)\n",
    "    \n",
    "    verizon_acs = verizon.merge(acs[acs_cols], how='left',\n",
    "                                left_on='block_group', right_on='geoid')\n",
    "    verizon_acs.to_csv(fn_verizon, index=False, compression='gzip')\n",
    "else:\n",
    "    verizon_acs = pd.read_csv(fn_verizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[datetime.datetime(2022, 4, 19, 17, 4, 55),\n",
       " datetime.datetime(2022, 4, 27, 23, 47, 27)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start and end collection datetime\n",
    "[datetime.fromtimestamp(verizon_acs.collection_datetime.min()), \n",
    " datetime.fromtimestamp(verizon_acs.collection_datetime.max())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "states.extend(verizon_acs['state'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Earthlink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(fn_el) or recalculate:\n",
    "    data_el = []\n",
    "    files = glob.glob(pattern_el)\n",
    "\n",
    "    with Pool(n_jobs) as pool:\n",
    "        for record in tqdm(pool.imap_unordered(el_workflow, files), \n",
    "                           total=len(files)):\n",
    "            data_el.extend(record)\n",
    "    el = pd.DataFrame(data_el)\n",
    "    del data_el\n",
    "    \n",
    "    el['block_group'] = el['block_group'].apply(lambda x: f\"{int(x):012d}\")\n",
    "    el = check_redlining(el)\n",
    "    el = get_closest_fiber(el)\n",
    "    \n",
    "    el_acs = el.merge(acs[acs_cols], how='left', \n",
    "                      left_on='block_group', right_on='geoid')\n",
    "    el_acs = el_acs[el_acs.incorporated_place.isin(inc_city_el)]\n",
    "    el_acs.to_csv(fn_el, index=False, compression='gzip')\n",
    "else:\n",
    "    el_acs = pd.read_csv(fn_el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[datetime.datetime(2022, 4, 20, 1, 45, 42),\n",
       " datetime.datetime(2022, 5, 25, 17, 1, 44)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start and end collection datetime\n",
    "[datetime.fromtimestamp(el_acs.collection_datetime.min()), \n",
    " datetime.fromtimestamp(el_acs.collection_datetime.max())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many states did we collect data from?\n",
    "states.extend(el_acs['state'].unique())\n",
    "len(set(states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AT&T           278236\n",
       "CenturyLink    112461\n",
       "Frontier         8376\n",
       "Name: contract_provider, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which ISPs is EarthLink leasing from?\n",
    "el_acs[el_acs.speed_down != 0].contract_provider.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chicago city                   0.104066\n",
       "Houston city                   0.095431\n",
       "Los Angeles city               0.081363\n",
       "Phoenix city                   0.058381\n",
       "Detroit city                   0.046281\n",
       "Jacksonville city              0.041479\n",
       "Charlotte city                 0.036511\n",
       "Portland city                  0.034805\n",
       "Seattle city                   0.033257\n",
       "Oklahoma City city             0.032477\n",
       "Memphis city                   0.031268\n",
       "Kansas City city               0.030919\n",
       "Las Vegas city                 0.030510\n",
       "Denver city                    0.030053\n",
       "New Orleans city               0.028419\n",
       "Milwaukee city                 0.027601\n",
       "Omaha city                     0.027394\n",
       "Indianapolis city (balance)    0.026712\n",
       "Albuquerque city               0.024980\n",
       "Cleveland city                 0.024481\n",
       "Name: incorporated_place, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "el_acs.incorporated_place.value_counts(normalize=True).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fiber Based    162562\n",
       "Fiber          134496\n",
       "Copper         102015\n",
       "Name: technology, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "el_acs.technology.value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
