{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Census Data\n",
    "Collects ACS data from census.gov API and merges it into one table.\n",
    "\n",
    "If you want to collect your own census data, you'll need to regiester for an [API key](https://api.census.gov/data/key_signup.html), and assign it as the environment variable `CENSUS_API_KEY`. \n",
    "\n",
    "This is not necessary, as all outputs are calcualted and saved in this repository already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import glob as glob\n",
    "import gzip\n",
    "import multiprocess\n",
    "from multiprocess import Pool\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download ACS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ouputs\n",
    "data_dir = '../data/input/census/acs5'\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# inputs\n",
    "fn_income = '../data/input/census/state2income.json'\n",
    "input_files = glob.glob('../data/input/isp/*/*/*.geojson.gz')\n",
    "fips = set([_.split('/')[-1].split('.geojson')[0][:5] for _ in input_files])\n",
    "fips = set([_ for _ in fips if _ != 'spotc'])\n",
    "state2income = json.load(open(fn_income, 'r'))\n",
    "\n",
    "# params\n",
    "recalculate = False\n",
    "n_jobs = 8\n",
    "API_KEY = os.environ.get('CENSUS_API_KEY')\n",
    "# assert API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making API calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs\n",
    "fn_out_acs = '../data/intermediary/census/aggregated_tables.csv.gz'\n",
    "os.makedirs(os.path.dirname(fn_out_acs), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the three ACS tables and columns we're going to get.\n",
    "acs_tables = [\n",
    "    {\n",
    "        \"display_name\": \"race_ethnicity\",\n",
    "        \"table_name\": \"B03002\",\n",
    "        \"url\": \"https://api.census.gov/data/2018/acs/acs5/groups/B03002.html\",\n",
    "        \"columns\": {\n",
    "            \"block group\": \"block_group\",\n",
    "            \"B03002_001E\": \"race_total_estimate\",\n",
    "            \"B03002_003E\": \"race_white_alone\",\n",
    "            \"B03002_004E\": \"race_black_alone\",\n",
    "            \"B03002_005E\": \"race_aindian_alone\",\n",
    "            \"B03002_006E\": \"race_asian_alone\",\n",
    "            \"B03002_007E\": \"race_pacific_islander_native_hawaiian_alone\",\n",
    "            \"B03002_008E\": \"race_some_other_alone\",\n",
    "            \"B03002_009E\": \"race_two_or_more_alone\",\n",
    "            \"B03002_012E\": \"race_latino_alone\",\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"display_name\": \"household_income_2\",\n",
    "        \"table_name\": 'B19013',\n",
    "        \"url\": \"https://api.census.gov/data/2018/acs/acs5/groups/B19013.html\",\n",
    "        \"columns\": {\n",
    "            \"block group\": \"block_group\",\n",
    "            \"B19013_001E\": \"median_household_income\",\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"display_name\": \"internet_subscription\",\n",
    "        \"table_name\": 'B28002',\n",
    "        \"url\": \"https://api.census.gov/data/2018/acs/acs5/groups/B28002.html\",\n",
    "        \"columns\": {\n",
    "            \"block group\": \"block_group\",\n",
    "            \"B28002_001E\": \"internet_total_estimate\",\n",
    "            \"B28002_002E\": \"internet_subscriptions_any\",\n",
    "            \"B28002_004E\": \"internet_broadband\",\n",
    "            \"B28002_005E\": \"internet_mobile\",\n",
    "            \"B28002_006E\": \"internet_mobile_only\",\n",
    "            \"B28002_013E\": \"internet_no_access\",\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_acs(column: str= \"B03002_001E\",\n",
    "                 geography: str = \"block group\",\n",
    "                 year: int= 2019, \n",
    "                 state: int= 55,\n",
    "                 county: [int, str]= 1,\n",
    "                 data_dir: str= 'data/',\n",
    "                 api_key: str= None,\n",
    "                 debug: bool= False) -> dict:\n",
    "    \"\"\"\n",
    "    geography can also be \"tract\",\n",
    "    column is the ACS \"table name\"\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import requests\n",
    "    \n",
    "    def make_acs_request(year, column, geography, state, county, api_key, debug=False):\n",
    "        \"\"\"\n",
    "        Formats the API call and make the reuqest\n",
    "        \"\"\"\n",
    "        county = str(county).zfill(3)\n",
    "        url = (f\"https://api.census.gov/data/{year}/acs/acs5?get={column}&\"\n",
    "               f\"for={geography}:*&in=state:{state}%20county:{county}&key={api_key}\")\n",
    "        if debug:\n",
    "            print(url)\n",
    "        return requests.get(url)\n",
    "\n",
    "    \n",
    "    # check if the file exists...\n",
    "    table = column.split('_')[0]\n",
    "    geography_ = geography.replace(' ', '_')\n",
    "    fn_out = f\"{data_dir}/{geography_}/{year}/{state}/{county}/{table}/{column}.csv.gz\"\n",
    "    if os.path.exists(fn_out):\n",
    "        return 1\n",
    "    os.makedirs(os.path.dirname(fn_out), exist_ok=True)\n",
    "    print(f\"collecting {fn_out}\")\n",
    "    # make the request\n",
    "    resp = make_acs_request(year, column, geography, state, county, api_key, debug)\n",
    "    \n",
    "    # validate the response and save it\n",
    "    if resp.status_code != 200:\n",
    "        pd.DataFrame([]).to_csv(fn_out, index=False, compression='gzip')\n",
    "        return 0\n",
    "    _data = resp.json()\n",
    "    _df = pd.DataFrame(_data[1:], columns=_data[0])\n",
    "    _df.to_csv(fn_out, index=False, compression='gzip')\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_acs_request(year, column, geography, state, county, api_key, debug=False):\n",
    "        \"\"\"\n",
    "        Formats the API call and make the reuqest\n",
    "        \"\"\"\n",
    "        county = str(county).zfill(3)\n",
    "        url = (f\"https://api.census.gov/data/{year}/acs/acs5?get={column}&\"\n",
    "               f\"for={geography}:*&in=state:{state}%20county:{county}&key={api_key}\")\n",
    "        if debug:\n",
    "            print(url)\n",
    "        return requests.get(url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To speed things up, we're going to use multiprocessing to collect these files.\n",
    "First, we must create a list of arguments for each API request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We must make 1568 API calls.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year = 2019\n",
    "debug = False\n",
    "args = []\n",
    "for fip in fips:\n",
    "    state = fip[:2]\n",
    "    county = fip[2:]\n",
    "    for geography in ['block group']:\n",
    "        for table in acs_tables:\n",
    "            table_name = table[\"display_name\"]\n",
    "            columns = [c for c in table[\"columns\"].keys() if c != \"block group\"]\n",
    "            for column in columns:\n",
    "                args.append([column, geography, year, state, county, data_dir, API_KEY, debug])\n",
    "\n",
    "f\"We must make {len(args)} API calls.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1568/1568 [00:00<00:00, 4504.71it/s]\n"
     ]
    }
   ],
   "source": [
    "# use multiprocessing to make the requests...\n",
    "def collect_data():\n",
    "    with multiprocess.get_context(\"spawn\").Pool(n_jobs) as pool:\n",
    "        pool.starmap(download_acs, tqdm(args, total=len(args)))\n",
    "        \n",
    "# only make API requests if the output file doesn't exists or if we want to recalculate.\n",
    "if not os.path.exists(fn_out_acs) or recalculate:\n",
    "    collect_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing\n",
    "With the ACS data collected, we're now going to calculate the racial demographics, income levels, and broadband pentration for each block group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_block_groups(verbose=False):\n",
    "    \"\"\"Get the block groups in the data we have\"\"\"\n",
    "    files_input = glob.glob(\"../data/input/isp/*/*/*.geojson.gz\")    \n",
    "    block_groups = set([f.split('/')[-1].split('.geojson')[0] for f in files_input])\n",
    "    block_groups = set([bg for bg in block_groups if len(bg) == 12])\n",
    "    if verbose:\n",
    "        print(f\"Found {len(block_groups)} block groups.\")\n",
    "    \n",
    "    return block_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_geoid(row):\n",
    "    \"\"\"\n",
    "    Explictly cast block group-level geoid\n",
    "    \"\"\"\n",
    "    state = int(row['state'])\n",
    "    county = int(row['county'])\n",
    "    tract = int(row['tract'])\n",
    "    block = int(row['block_group'])\n",
    "    \n",
    "    return f\"{state:02}{county:03}{tract:06}{block:01}\"\n",
    "\n",
    "def percent_non_white(row):\n",
    "    \"\"\"\n",
    "    Percentage of area that are not non-Hispanic white\n",
    "    \"\"\"\n",
    "    total = row['race_total_estimate']\n",
    "    white = row['race_white_alone']\n",
    "    try:\n",
    "        perc_non_white =  (total - white) / total\n",
    "        return max(0, min(perc_non_white, 1))\n",
    "    except ZeroDivisionError:\n",
    "        return None\n",
    "    \n",
    "def income_level(row):\n",
    "    \"\"\"\n",
    "    Calculate median household income compared to city median income.\n",
    "    We set None for skip values set by the census bureau.\n",
    "    \"\"\"\n",
    "    state = row['state']\n",
    "    if row['median_household_income'] == -666666666:\n",
    "        return None\n",
    "    median_income = state2income.get(str(state), {})\n",
    "    if median_income:\n",
    "        return row['median_household_income'] / median_income\n",
    "    return None\n",
    "\n",
    "def dollars_diff_median(row):\n",
    "    \"\"\"\n",
    "    Get dollars below median city income.\n",
    "    \"\"\"\n",
    "    state = row['state']\n",
    "    if row['median_household_income'] == -666666666:\n",
    "        return None\n",
    "    median_income = state2income.get(str(state), {})\n",
    "    if median_income:\n",
    "        return median_income - row['median_household_income']\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(fn_out_acs) or recalculate:\n",
    "    # This merges and concats each ACS table we collected.\n",
    "    geography = \"block group\"\n",
    "    df = pd.DataFrame([])\n",
    "    for table in acs_tables:\n",
    "        col2rename = table['columns']\n",
    "        table_name = table['table_name']\n",
    "        cols = [c for c in col2rename.keys() if c != \"block group\"]\n",
    "        for column in tqdm(cols):\n",
    "            files = glob.glob(f\"../data/input/census/acs5/{geography.replace(' ', '_')}/{year}/*/*/{table_name}/{column}.csv.gz\")\n",
    "            data = []\n",
    "            for fn in files:\n",
    "                tp = pd.read_csv(fn, compression='gzip')\n",
    "                if tp.empty:\n",
    "                    print(fn)\n",
    "                data.extend(tp.to_dict(\"records\"))\n",
    "            tmp = pd.DataFrame(data)\n",
    "            tmp.columns = [col2rename.get(c, c) for c in tmp.columns]\n",
    "            if df.empty:\n",
    "                df = tmp\n",
    "            else:\n",
    "                df = df.merge(tmp, on= [\"state\", \"county\", \"tract\", \"block_group\"], how='outer')\n",
    "\n",
    "    # process the data\n",
    "    df['race_perc_non_white'] = df.apply(percent_non_white, axis=1)\n",
    "    df['internet_perc_broadband'] = df['internet_broadband'] / df['internet_total_estimate']\n",
    "    df['geoid'] = df.apply(get_geoid, axis=1)\n",
    "    \n",
    "    # For whatever reason, calling these functions can return None.\n",
    "    # check how many null values they produce, and run again if there are a lot of nulls.\n",
    "    df['income_dollars_below_median'] = df.apply(dollars_diff_median, axis=1)\n",
    "    df['income_lmi'] = df.apply(income_level, axis=1)\n",
    "    \n",
    "    df.to_csv(fn_out_acs, index=False, compression='gzip')\n",
    "else:\n",
    "    df = pd.read_csv(fn_out_acs, compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing raw shape file for maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TIGER](https://www.census.gov/geographies/mapping-files/time-series/geo/tiger-line-file.2019.html) shape files were downloaded and stored in one place (`../data/input/census/shape/raw`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ouputs\n",
    "fn_out_shp = '../data/intermediary/census/2019_acs_5_shapes.geojson.gz'\n",
    "pattern_geojson = '../data/input/census/shape/preprocessed/*.geojson'\n",
    "\n",
    "# inputs\n",
    "files_shape = glob.glob('../data/input/census/shape/raw/tl_2019_*_bg.zip')\n",
    "len(files_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to filter out block groups that aren't in our investigation, merge ACS demographic data, and combine the shapes into one file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the zip files need to be converted into geoJSON (via geopandas `gpd`) to map-able."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46/46 [00:00<00:00, 42061.91it/s]\n"
     ]
    }
   ],
   "source": [
    "# convert zip files to geoJSON\n",
    "for fn in tqdm(files_shape):\n",
    "    fn = os.path.abspath(fn)\n",
    "    fn_out = fn.replace('/raw/', '/preprocessed/').replace('.zip', '.geojson')\n",
    "    if os.path.exists(fn_out):\n",
    "        continue\n",
    "    os.makedirs(os.path.dirname(fn_out), exist_ok=True)\n",
    "    shp = gpd.read_file('zip:///' + fn)\n",
    "    shp.to_file(fn_out, driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(fn_out_shp) or recalculate:\n",
    "    df = pd.read_csv(fn_out_acs, compression='gzip')\n",
    "    df.geoid = df.geoid.apply(lambda x: f\"{x:012d}\")\n",
    "\n",
    "    block_groups = get_relevant_block_groups(verbose=True)    \n",
    "    geoid2meta = {row['geoid']: {\n",
    "        'race_perc_non_white' : row['race_perc_non_white'], \n",
    "        'income_dollars_below_median' : row['income_dollars_below_median'],\n",
    "        'median_household_income' : row['median_household_income']\n",
    "    }  for i, row in df[df.geoid.isin(block_groups)].iterrows()}\n",
    "    len(geoid2meta)\n",
    "    \n",
    "    save_all = []\n",
    "    files_geojson = glob.glob(pattern_geojson)\n",
    "    geography = \"block group\"\n",
    "    for fn_geojson in tqdm(files_geojson):\n",
    "        fn_out =  f'../data/intermediary/maps/acs/{os.path.basename(fn_geojson)}'\n",
    "        if not os.path.exists(fn_out) or recalculate:\n",
    "            os.makedirs(os.path.dirname(fn_out), exist_ok=True)\n",
    "            to_save = []\n",
    "            with open(fn_geojson , 'r') as f:\n",
    "                geojson = json.load(f)\n",
    "                for i, feature in enumerate(geojson['features']):\n",
    "                    featureProperties = feature['properties']\n",
    "                    if geography == \"block group\":\n",
    "                        geoid = featureProperties['GEOID']\n",
    "                    featureData = geoid2meta.get(geoid, {})\n",
    "                    for key in featureData.keys():\n",
    "                        featureProperties[key] = featureData[key]\n",
    "                    if featureData:\n",
    "                        to_save.append(feature)\n",
    "                        save_all.append(feature)\n",
    "\n",
    "                # save geoJSON with acs data merged in\n",
    "                geojson['features'] = to_save\n",
    "                with open(fn_out, 'w') as f:\n",
    "                    f.write(json.dumps(geojson).replace('NaN', '\"\"'))\n",
    "    geojson['features'] = save_all\n",
    "    with gzip.open(fn_out_shp, 'wt') as f:\n",
    "        f.write(json.dumps(geojson).replace('NaN', '\"\"'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## population density & number of competing ISPs\n",
    "Final tweaks to the ACS data to be used for analysis.\n",
    "\n",
    "We reference TIGER shape files to calculate population density using the the area of land for each census block group and the estimated population. We also merge the number of competing ISPs for each block group according to the FCC's Form 477."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ouputs\n",
    "fn_out_features = '../data/intermediary/census/aggregated_tables_plus_features.csv.gz'\n",
    "fn_providers = '../data/intermediary/fcc/bg_providers.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "fn_form_477 = '../data/input/fcc/fbd_us_with_satellite_dec2020_v1.csv.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of competitors from FCC Form 477.\n",
    "if not os.path.exists(fn_providers) or recalculate:\n",
    "    # Filter for block groups in our invetigation.\n",
    "    block_groups = get_relevant_block_groups(verbose=True)\n",
    "    data = []\n",
    "    for _df in pd.read_csv(fn_form_477, \n",
    "                           compression= 'gzip',\n",
    "                           encoding= 'unicode_escape', \n",
    "                           chunksize= 50000, \n",
    "                           dtype= {'BlockCode' : str}):\n",
    "        _df['block_group'] = _df['BlockCode'].apply(lambda x: x[:12])\n",
    "        data.extend(_df[_df['block_group'].isin(block_groups)]\n",
    "                                          .to_dict(orient='records'))\n",
    "    df_bg = pd.DataFrame(data)\n",
    "    \n",
    "    (df_bg[\n",
    "        (df_bg.Consumer == 1) &\n",
    "        (~df_bg.TechCode.isin([60,70]))   \n",
    "    ].groupby('block_group')['ProviderName']\n",
    "     .nunique().to_frame('n_providers')\n",
    "     .to_csv(fn_providers)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_persons_per_sq_mile(row):\n",
    "    \"\"\"\n",
    "    See formula referenced in these two sources:\n",
    "    https://acsdatacommunity.prb.org/discussion-forum/f/forum/585/moe-for-population-density\n",
    "    https://www.census.gov/quickfacts/fact/note/US/LND110210\n",
    "    1,000,000 * POP / ALAND\n",
    "    \"\"\"\n",
    "    if row['ALAND'] == 0:\n",
    "        return None\n",
    "    sq_miles = row['ALAND'] / 1000000\n",
    "    persons = row['race_total_estimate']\n",
    "    return persons / sq_miles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(fn_out_features) or recalculate:\n",
    "    df = pd.read_csv(fn_out_acs, compression='gzip', dtype={'geoid': str})\n",
    "    df_shp = pd.DataFrame([row['properties'] for row in \n",
    "                           json.load(gzip.open(fn_out_shp, 'r'))['features']])\n",
    "    df_providers = pd.read_csv(fn_providers, dtype={'block_group': str})\n",
    "    \n",
    "    df = df.merge(df_shp[['GEOID', 'ALAND']], \n",
    "                  how='left', left_on='geoid', right_on='GEOID', \n",
    "                  suffixes=('', '_DROP')).filter(regex='^(?!.*_DROP)')\n",
    "    df['ppl_per_sq_mile'] = df.apply(calculate_persons_per_sq_mile, axis=1)\n",
    "\n",
    "    df = df.merge(df_providers, \n",
    "                  how='left', left_on='GEOID', right_on='block_group', \n",
    "                  suffixes=('', '_DROP')).filter(regex='^(?!.*_DROP)')\n",
    "    \n",
    "    df.to_csv(fn_out_features, index=False, compression='gzip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
